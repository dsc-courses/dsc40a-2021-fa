{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7 – More Simple Linear Regression\n",
    "\n",
    "## DSC 40A, Fall 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML, Math\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load in our dataset. Don't worry about what it's doing.\n",
    "np.random.seed(25)\n",
    "\n",
    "salaries_raw = pd.read_csv('data/data_scientist_salaries.csv')\n",
    "salaries = salaries_raw.get(['YearsCodingProf', 'Age', 'FormalEducation', 'Salary']).dropna()\n",
    "\n",
    "def extract_years(year_str):\n",
    "    if isinstance(year_str, float):\n",
    "        return year_str\n",
    "    if 'older' in year_str:\n",
    "        years = 65\n",
    "    elif 'more' in year_str:\n",
    "        years = 30\n",
    "    elif 'Under' in year_str:\n",
    "        years = 18\n",
    "    else:\n",
    "        extracted = re.findall('\\d+', year_str)\n",
    "        try:\n",
    "            lower, upper = int(extracted[0]), int(extracted[1])\n",
    "        except:\n",
    "            print(extracted)\n",
    "        years = np.random.randint(lower, upper + 1)\n",
    "    return years + np.round(np.random.normal(0, 1), 2)\n",
    "\n",
    "salaries['Age'] = salaries['Age'].apply(extract_years)\n",
    "salaries['YearsExperience'] = salaries['YearsCodingProf'].apply(extract_years)\n",
    "salaries = salaries[['YearsExperience', 'Age', 'FormalEducation', 'Salary']]\n",
    "salaries = salaries[(salaries['Salary'] < 500000) & (salaries['Salary'] > 1000) & (salaries['YearsExperience'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(salaries, x='YearsExperience', y='Salary', title='Salary vs. Years of Experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(salaries.get('Salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(salaries.get('Salary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "$$\\begin{align*} r &= \\text{the average of the product of $x$ and $y$, when both are in standard units} \\\\ &= \\frac{1}{n} \\sum_{i = 1}^n \\left( \\frac{x_i - \\bar{x}}{\\sigma_x} \\right) \\left( \\frac{y_i - \\bar{y}}{\\sigma_y} \\right)  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    x_su = (x - np.mean(x)) / np.std(x)\n",
    "    y_su = (y - np.mean(y)) / np.std(y)\n",
    "    \n",
    "    return np.mean(x_su * y_su)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(salaries.get('YearsExperience'))\n",
    "ys = np.array(salaries.get('Salary')) / 1000 # Will measure salary in 1000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric!\n",
    "correlation(ys, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't change if we multiply x or y by constants!\n",
    "correlation(xs * 1000, ys * 545)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames in pandas have a built-in correlation method\n",
    "salaries.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing $w_0^*$ and $w_1^*$\n",
    "\n",
    "Recall, the formulas for the optimal intercept and slope are\n",
    "\n",
    "$$w_1^* = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "$$w_0^* = \\bar{y} - w_1^* \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y) * np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_star = intercept(xs, ys)\n",
    "w1_star = slope(xs, ys)\n",
    "\n",
    "# Just fancy printing – ignore these next two lines.\n",
    "rule_string = '$$\\\\text{Predicted Salary (in \\$1000s)} = ' + f'{int(w0_star)} + {int(w1_star)}' + '\\cdot \\\\left( \\\\text{Years of Experience} \\\\right)$$'\n",
    "display(HTML(f'<h4>The best linear predictor, under squared loss, for this dataset is</h4><br><center>{rule_string}</center>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(salaries, x='YearsExperience', y='Salary', title='Salary vs. Years of Experience')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = xs, y = ys, mode = 'markers', name = 'actual'))\n",
    "fig.add_trace(go.Scatter(x = xs, y = w0_star + w1_star * xs, name = 'linear prediction rule', line=dict(color='red')))\n",
    "fig.update_layout(xaxis_title = 'Years of Experience', yaxis_title = 'Salary ($1000s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have $w_0^*$ and $w_1^*$, we can use them to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_salary(yoe):\n",
    "    return w0_star + w1_star * yoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_salary(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_salary(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_salary(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak into the future – how do we use linear regression in practice?\n",
    "\n",
    "In practice, most of this stuff is already implemented by various packages. The goal of discussing loss functions and empirical risk is to show you how it all works.\n",
    "\n",
    "One of the more common packages in Python for machine learning work is `scikit-learn`, also called `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xs.reshape(-1, 1), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these numbers match those from our manual calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_star, w1_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry at all about how `sklearn` works – that's for DSC 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does $R_{sq}(w_0, w_1)$ look like?\n",
    "\n",
    "Let's draw a plot of $R_{sq}(w_0, w_1)$, the empirical risk that we're trying to minimize.\n",
    "- When we only had a single parameter, $h$, $R(h)$ was in 2D.\n",
    "    - One axis for $h$, one axis for $R(h)$.\n",
    "- Now that we have two parameters, $w_0$ and $w_1$, $R(h)$ will be in 3D!\n",
    "    - One axis for $w_0$, one axis for $w_1$, one axis for $R(h)$.\n",
    "    - The x-y plane consists of all possible combinations of slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(w):\n",
    "    w0 = w[0]\n",
    "    w1 = w[1]\n",
    "    return np.mean((ys - (w0 + w1 * xs))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mse(mean_squared_error, [w0_star, w1_star], show_min=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: pitfalls of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe = pd.read_csv('data/anscombe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, n in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    rows = anscombe[anscombe.get('dataset') == n]\n",
    "    x = rows['x']\n",
    "    y = rows['y']\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(x, y, label=f'Dataset {n}', alpha=0.65, s=65)\n",
    "    plt.title(f'Dataset {n}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do all four of these datasets have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    rows = anscombe[anscombe.get('dataset') == n]\n",
    "    x = rows['x']\n",
    "    y = rows['y']\n",
    "    \n",
    "    r = correlation(x, y)\n",
    "    outstr = f'''\n",
    "    <b>Dataset {n}</b><br>\n",
    "    $\\\\bar x$: {np.round(np.mean(x), 2)}<br>\n",
    "    $\\\\bar y$: {np.round(np.mean(y), 2)}<br>\n",
    "    $\\\\sigma_x$: {np.round(np.std(x), 2)}<br>\n",
    "    $\\\\sigma_y$: {np.round(np.std(y), 2)}<br>\n",
    "    $r$: {np.round(r, 2)}\n",
    "    '''\n",
    "    display(HTML(outstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all share the exact same mean and standard deviation of $x$ and $y$, and the same correlation coefficient $r$! This means they all have the same best linear prediction rule, under squared loss.\n",
    "\n",
    "However, that linear prediction rule looks better for some datasets than it does for others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, n in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    rows = anscombe[anscombe.get('dataset') == n]\n",
    "    x = rows['x']\n",
    "    y = rows['y']\n",
    "    \n",
    "    w0_ans = intercept(x, y)\n",
    "    w1_ans = slope(x, y)\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(x, y, label=f'Dataset {n}', alpha=0.65, s=65)\n",
    "    plt.plot(x, w0_ans + w1_ans * x, color='red');\n",
    "    plt.title(f'Dataset {n}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the story – visualize your data before trying to fit a prediction rule!\n",
    "\n",
    "If that was interesting, [check out this article](https://www.autodesk.com/research/publications/same-stats-different-graphs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
