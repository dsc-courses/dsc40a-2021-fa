{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 â€“ Gradient Descent\n",
    "\n",
    "## DSC 40A, Fall 2021\n",
    "\n",
    "In this notebook, we'll see gradient descent in action. This demo is intended to be a *supplement* to lecture; in particular, you won't be expected to code up gradient descent. In addition, the code in this notebook is not designed to be *fast*, but instead *easy to understand*. Remember: the focus of DSC 40A is more on the math than the code.\n",
    "\n",
    "We'll get started by importing the familiar Python numerical and plotting packages, `numpy` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import ipywidgets\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "mpl.rcParams['figure.figsize'] = [12, 8]\n",
    "mpl.rcParams['animation.html'] = 'jshtml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code for Gradient Descent\n",
    "\n",
    "First, we'll code up a general gradient descent function. The code below implements gradient descent as seen in lecture. **Don't worry about the details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "        derivative_of_f, *, h_0, alpha, max_iter=10_000, tol=1e-12, verbose=False,\n",
    "        callback=None\n",
    "    ):\n",
    "    \"\"\"Minimize a (univariate) function f using gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    derivative_of_f : callable\n",
    "        A function which accepts one argument, h, and outputs the derivative\n",
    "        of f at h.\n",
    "    h_0 : float\n",
    "        The initial guess of the minimizer.\n",
    "    alpha : float\n",
    "        The step size parameter.\n",
    "    max_iter : int\n",
    "        The maximum number of steps to take.\n",
    "    tol : float\n",
    "        The convergence tolerance. If the difference between subsequent guesses\n",
    "        is less than tol, the algorithm will assume that it has converged.\n",
    "    verbose : bool\n",
    "        If `True`, prints the progress of the search.\n",
    "    callback : callable\n",
    "        A function called after every update with the new position.\n",
    "    \"\"\"\n",
    "    h = h_0\n",
    "    for iteration in range(max_iter):\n",
    "        h_next = h - alpha * derivative_of_f(h)\n",
    "        if np.abs(h_next - h) < tol:\n",
    "            break\n",
    "        if verbose:\n",
    "            print(f'iter #{iteration}: h={h_next}')\n",
    "        if callback is not None:\n",
    "            callback(h_next)\n",
    "        h = h_next\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('Reached Max Iters')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StackOverflow Salary Data\n",
    "\n",
    "Let's go back to the problem of predicting a data scientist's salary. We'll do this by minimizing empirical risk with gradient descent.\n",
    "\n",
    "First, we'll load the data with `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "SALARIES = pd.read_csv('data_scientist_salaries.csv').get('Salary').values\n",
    "SALARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SALARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how the salaries are distributed, we'll plot a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(SALARIES, bins=30);\n",
    "plt.xlabel('Salary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few outliers, and they're making it hard to see the shape of the distribution. If -- just for the plot -- we remove the salaries above $500,000, we see get a clearer picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(SALARIES[SALARIES < 500_000], bins=30);\n",
    "plt.xlabel('Salary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the \"center\" of the distribution is around $100,000. In fact, the mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(SALARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the median is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(SALARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Minimizing the mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our first prediction by minimizing the mean squared error.\n",
    "\n",
    "Recall that the squared loss is $L_\\text{sq}(h,y) = (y-h)^2$. Given a data set of numbers, $y_1, \\ldots, y_n$, the **empirical risk** associated with this loss is:\n",
    "$$\n",
    "    R_\\text{sq}(h) = \\frac1n \\sum_{i=1}^n L_\\text{sq}(h,y_i)\n",
    "    =\n",
    "    \\frac1n \\sum_{i=1}^n (y_i - h)^2\n",
    "$$\n",
    "When the squared loss is used, the empirical risk is also known as the **mean squared error**. We saw in Lecture 3 that the mean squared error is directly minimized by the **mean**.\n",
    "\n",
    "But we can also minimize the mean squared error with gradient descent. Why would we do this, given that we have a nice, simple formula for the minimizer? In truth, we wouldn't. But in coming weeks we will see situations where we also have a formula for the minimizer, but we decide to use gradient descent instead. In those situations, it turns out that gradient descent is computationally faster.\n",
    "\n",
    "To begin, let's plot the mean squared error (MSE) for a range of predictions. We start by defining a function that computes the MSE for any possible prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def mean_squared_error(h):\n",
    "    return np.mean((SALARIES - h)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the MSE of predicting $h = \\$100{,}000$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate the MSE on each of 1,000 possible predictions, from $\\$25{,}000$ to $\\$200{,}000$, and plot the empirical risk.\n",
    "We'll also include the histogram of the data in the background so that we can see how the MSE relates to the data's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_predictions = np.linspace(25_000, 200_000, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_risk_and_histogram(risk, y_label='MSE'):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.hist(SALARIES[SALARIES<300_000], bins=20, alpha=.5, zorder=1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.grid(False)\n",
    "    ax2.plot(possible_predictions, risk(possible_predictions), color='black', zorder=20)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_risk_and_histogram(mean_squared_error)\n",
    "plt.scatter(np.mean(SALARIES), mean_squared_error(np.mean(SALARIES)), color='black', marker='^');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the salaries is shown as the black triangle. As we expected, the minimizer of the mean squared error is indeed the mean. But instead of computing the mean directly, let's minimize this function using gradient descent.\n",
    "\n",
    "To do so, we need a function which returns the derivative of $R_\\text{sq}$ at any given place. Recall that the derivative of $R_\\text{sq}$ with respect to $h$ is:\n",
    "\n",
    "$$\n",
    "    \\frac{dR_\\text{sq}}{dh}(h)\n",
    "    =\n",
    "    \\frac{2}{n}\n",
    "    \\sum_{i=1}^n\n",
    "        (h - y_i)\n",
    "$$\n",
    "\n",
    "Let's code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def derivative_of_mean_squared_error(h):\n",
    "    return 2 * np.mean(h - SALARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the derivative of the MSE at any point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_of_mean_squared_error(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that we got the derivative right, we can check that it gives the right slope. The cell below defined a couple of helper functions which use `derivative_of_mean_squared_error` to plot the tangent line the the MSE at any point we choose. If the derivative is correct, we should see a line that is indeed tangent to the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_tangent_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tangent_line(\n",
    "    f=mean_squared_error,\n",
    "    f_prime=derivative_of_mean_squared_error,\n",
    "    interval_size=30_000,\n",
    "    domain=possible_predictions,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We know have everything we need to run gradient descent. We'll start with a step size of $\\alpha = .1$, and we'll use $h_0 = 60{,}000$ as our starting position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gradient_descent(\n",
    "    derivative_of_mean_squared_error,\n",
    "    h_0=60_000,\n",
    "    alpha=.1,\n",
    "    verbose=True,\n",
    "    tol=.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty close to the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(SALARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let gradient descent run for even longer it would get arbitrarily close to the mean. So we have successfully computed the mean without using the formula for the mean.\n",
    "\n",
    "Let's visualize the progress of gradient descent. The code to make the visualization is somewhat long and besides the point, so I've put it in a utility module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below is an animation. The current position of gradient descent is shown as a black dot. The arrow shows the update step -- it's length is exactly $-\\alpha \\frac{dR}{dh}(h)$. The dashed line is the tangent line, and the blue dot is the new position after updating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=mean_squared_error,\n",
    "    derivative_of_f=derivative_of_mean_squared_error,\n",
    "    h_0=60_000,\n",
    "    alpha=.1,\n",
    "    n_iters=15,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(50_000, 175_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a bigger step size of $\\alpha=.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=mean_squared_error,\n",
    "    derivative_of_f=derivative_of_mean_squared_error,\n",
    "    h_0=60_000,\n",
    "    alpha=.25,\n",
    "    n_iters=15,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(50_000, 175_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we converge even faster. How about if we use a step size of $\\alpha = 1.1?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=mean_squared_error,\n",
    "    derivative_of_f=derivative_of_mean_squared_error,\n",
    "    h_0=60_000,\n",
    "    alpha=1.1,\n",
    "    n_iters=15,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(25_000, 200_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent appears to diverge! This suggests that we must strike a balance when choosing the step size ($\\alpha$): too small and it will take a long time to converge. Too big, and we might diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Minimizing the mean UCSD loss\n",
    "\n",
    "In lecture, we designed our own loss function, which we called $L_\\text{ucsd}$:\n",
    "\n",
    "$$\n",
    "    L_\\text{ucsd}(h,y) = 1 - e^{-(y-h)^2 / \\sigma^2}\n",
    "$$\n",
    "\n",
    "Here, $\\sigma$ is a **scale parameter**; it determines what is considered to be an outlier. We can use this loss to make a prediction by minimizing the empirical risk:\n",
    "\n",
    "$$\n",
    "R_\\text{ucsd}(h) \n",
    "= \n",
    "\\frac{1}{n} \\sum_{i=1}^n L_\\text{ucsd}(h, y_i)\n",
    "=\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\left(1 - e^{-(y_i - h)^2 / \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "In class, we tried to do so by taking a derivative, setting to zero, and solving. The function was differentiable,\n",
    "with derivative:\n",
    "\n",
    "$$\n",
    "\\frac{dR_\\text{ucsd}}{dh}(h)\n",
    "=\n",
    "\\frac{2}{n\\sigma^2} \\sum_{i=1}^n (h- y_i) e^{-(y_i - h)^2/\\sigma^2}\n",
    "$$\n",
    "\n",
    "But when we went to solve this for $h$, we got stuck. Let's try to use gradient descent to minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's plot $R_\\text{ucsd}$. To do so, we first have to choose the value of the scale parameter, $\\sigma$. We'll work with three particular choices: $\\sigma_1 = 7500$, $\\sigma_2 = 10000$, and $\\sigma_3 = 30000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ucsd_risk(sigma):\n",
    "    @np.vectorize\n",
    "    def ucsd_risk(h):\n",
    "        return (1 - np.exp(-(h - SALARIES)**2 / sigma**2)).mean()\n",
    "    return ucsd_risk\n",
    "\n",
    "ucsd_risk_7_500 = make_ucsd_risk(7_500)\n",
    "ucsd_risk_10_000 = make_ucsd_risk(10_000)\n",
    "ucsd_risk_30_000 = make_ucsd_risk(30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_risk_and_histogram(ucsd_risk_7_500, y_label=r'$R_{ucsd}(h)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_risk_and_histogram(ucsd_risk_10_000, y_label=r'$R_{ucsd}(h)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_risk_and_histogram(ucsd_risk_30_000, y_label=r'$R_{ucsd}(h)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run gradient descent, we need to define a function that computes the derivative for any value of $h$. Again, we'll need a function for each choice of scale parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_derivative_of_ucsd_risk(sigma):\n",
    "    @np.vectorize\n",
    "    def derivative_of_ucsd_risk(h):\n",
    "        return ((h - SALARIES) * np.exp(-(h - SALARIES)**2 / sigma**2)).mean() * (2/sigma**2)\n",
    "    return derivative_of_ucsd_risk\n",
    "\n",
    "derivative_of_ucsd_risk_7_500 = make_derivative_of_ucsd_risk(7_500)\n",
    "derivative_of_ucsd_risk_10_000 = make_derivative_of_ucsd_risk(10_000)\n",
    "derivative_of_ucsd_risk_30_000 = make_derivative_of_ucsd_risk(30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tangent_line(\n",
    "    f=ucsd_risk_7_500,\n",
    "    f_prime=derivative_of_ucsd_risk_7_500,\n",
    "    interval_size=30_000,\n",
    "    domain=possible_predictions,\n",
    "    y_label=r'$R_{ucsd}(h)$'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to run gradient descent. Let's start with $\\sigma = 30{,}000$, and use a step size of $\\alpha = 0.1$; this worked well with the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_30_000,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_30_000,\n",
    "    h_0=140_000,\n",
    "    alpha=.1,\n",
    "    n_iters=10,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(25_000, 175_000, 1_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it isn't working. What could be wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "What is the most likely reason that gradient descent appears to be stuck?\n",
    "\n",
    "A. Our calculated derivative $\\frac{d}{dh} R_{ucsd}(h)$ is wrong.\n",
    "\n",
    "B. We have chosen a bad starting location $h_0$.\n",
    "\n",
    "C. We have chosen a bad step size $\\alpha$.\n",
    "\n",
    "D. Our formula for the empirical risk $R_{ucsd}(h)$ is wrong.\n",
    "\n",
    "### To answer, go to **[menti.com](https://menti.com)** and enter the code **7910 4287**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Answer</h3>\n",
    "<details>\n",
    "<summary>Click here to show.</summary>\n",
    "    \n",
    "<b>C. We have chosen a bad step size $\\alpha$.</b>\n",
    "    \n",
    "Look at the scale of the axes. Here, the horizontal axis is on the order of 100,000, while the vertical axis is on the order of 1. This means that a change in the prediction of one dollar only increases or decreases the empirical risk by a small amount. Before, with the MSE, the scale of the horizontal axis was the same, but the scale of the vertical axis was on the order of tens of billions. A change of one dollar caused the MSE to increase by a lot.\n",
    "\n",
    "This is supported by the derivatives:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_of_ucsd_risk_30_000(140_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_of_mean_squared_error(140_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the update is $\\alpha \\cdot (\\text{the derivative at $h$})$. Since the derivative is so small, we need to increase the step size to compensate. If we want to make a step of about 5,000 dollars, then we should set $\\alpha$ to be:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{5{,}000}{\\text{derivative at $h$}} = \\frac{5{,}000}{4 \\cdot 10^{-6}} \\approx 2 \\text{ billion}\n",
    "$$\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_30_000,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_30_000,\n",
    "    h_0=140_000,\n",
    "    alpha=2e9,\n",
    "    n_iters=10,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(25_000, 175_000, 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try $\\sigma = 7{,}500$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_7_500,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_7_500,\n",
    "    h_0=140_000,\n",
    "    alpha=2e9,\n",
    "    n_iters=15,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(40_000, 160_000, 1000),\n",
    "    arrow_height=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converged to a **local minimum**! That's not quite what we wanted. What if we use a larger step size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_7_500,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_7_500,\n",
    "    h_0=140_000,\n",
    "    alpha=7e9,\n",
    "    n_iters=50,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(40_000, 160_000, 1000),\n",
    "    arrow_height=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're bouncing around again! It is really hard to pick a step size that works when the function looks like this (there may not be one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, suppose we had started at $h_0 = 60{,}000$ instead of $h_0 = 140{,}000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_7_500,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_7_500,\n",
    "    h_0=60_000,\n",
    "    alpha=2e9,\n",
    "    n_iters=20,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(40_000, 160_000, 1000),\n",
    "    arrow_height=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks to have worked. Choosing different starting locations and running gradient descent a bunch of times is an actual strategy that people use to find the global minimum of a function with a bunch of local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the empirical risk with $\\sigma = 10{,}000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient_descent(\n",
    "    f=ucsd_risk_10_000,\n",
    "    derivative_of_f=derivative_of_ucsd_risk_10_000,\n",
    "    h_0=140_000,\n",
    "    alpha=4e9,\n",
    "    n_iters=50,\n",
    "    interval_size=15_000,\n",
    "    domain=np.linspace(40_000, 160_000, 1000),\n",
    "    arrow_height=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks to have worked, though it slowed down a bit when the function was flat. This is another difficulty with gradient descent: if the search enters a region where the function is near flat, it will slow down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, refer back to the lecture slides for a discussion on convexity, a mathematical property that will help explain when exactly gradient descent converges to a global minimum.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
