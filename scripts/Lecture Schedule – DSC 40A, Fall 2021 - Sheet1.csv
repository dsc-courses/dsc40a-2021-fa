Week,Date,#,Lecture,Narrative Notes,Lab,Homework
1,9/23,1,Introduction to Optimization,"Add that we could use lines of best fit, etc but we'll start simple.",,
2,9/28,2,Mean Absolute Error,,,
,9/30,3,"Mean Squared Error, Spread, The Modeling Process","At the end, reflect on what we did – we decided to use a constant hypothesis, chose a loss, and chose the optimal model.",,
3,10/5,4,"Other Loss Functions, Towards Gradient Descent",,,
,10/7,5,Gradient Descent and Convexity,,,
4,10/12,6,Simple Linear Regression,"Recap the modeling process. Introduce a more sophisticated model. Optimize w1, w0 by hand.",,
,10/14,7,"Simple Linear Regression, The Linear Algebra Perspective","Hmm, there's another way we can think about SLR – introduce matrices, etc. Conclude with a reformulated loss.",,
5,10/19,8,The Linear Algebra Perspective,"Spend the time deriving the solution to the normal equations. Why is this more powerful? Well, we can easily add more features.",,
,10/21,,Midterm (in lecture),Covers lectures 1-7.,,
6,10/26,9,Feature Engineering,Adding more features. Standardization. Non-linear regression. [add Amdahl's Law example here],,
,10/28,10,"Feature Engineering, Taxonomy of Machine Learning",Finishing whatever wasn't covered previously + one-hot encoding. ML taxonomy before k-means. [somewhere here bring up GD again],,
7,11/2,11,k-Means Clustering,,,
,11/4,12,Introduction to Probability; Set Theory,Ending the unit on prediction and moving towards probability. Set theory. PIE.,,
8,11/9,13,Combinatorics,"Permutations and combinations, and how they relate to probability. [can use DeCal notes]",,
,11/11,14,Conditional Probability,Review previous stuff. Bayes rule.,,
9,11/16,15,Independence,Will also involve review of 15.,,
,11/18,16,Naive Bayes,,,
10,11/23,17,Naive Bayes,,,
,11/25,,N/A (Thanksgiving),,,
11,11/30,18,Fun Stuff [simulations?],,,
,12/2,19,Review,,,
12,12/8,,Final Exam,,,