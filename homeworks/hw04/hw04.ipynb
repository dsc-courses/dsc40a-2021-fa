{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Supplemental Notebook\n",
    "\n",
    "## DSC 40A, Fall 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Here, we'll define several functions that you'll need to use in this notebook. **Don't reinvent the wheel, use the functions that are here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_normal_equations(X, y):\n",
    "    '''Returns the optimal parameter vector, w*, given a design matrix X and observation vector y.'''\n",
    "    return np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "def create_design_matrix(df, columns, intercept=True):\n",
    "    '''Creates a design matrix by taking the specified columns from the DataFrame df.\n",
    "       Adds a column of all 1s as the first column if intercept is True, which is the default.\n",
    "       The argument columns should be a list.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df['1'] = 1\n",
    "    if intercept:\n",
    "        return df[['1'] + columns].values\n",
    "    else:\n",
    "        return df[columns].values\n",
    "    \n",
    "def mean_squared_error(X, y, w):\n",
    "    '''Returns the mean squared error of the predictions Xw and observations y.'''\n",
    "    return np.mean((y - X @ w)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 â€“ Billy's Back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** While this problem seems quite long, the amount of work you have to do is quite minimal. Most of the code has already been implemented for you, you will generally just need to tweak a few things and interpret the results. You will see the text <a style=\"color:red\"><b>Your Job</b></a> next to each of your action items. You will not have to submit this notebook anywhere; each subpart will specify what you need to include in your PDF writeup.\n",
    " \n",
    "\n",
    "Run the cell below to load in a dataset containing information about the tips Billy, our avocado farmer from Homework 3 and the Midterm, received over the last month as a waiter at Dirty Birds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips = pd.read_csv('data/billy_tips.csv')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to a single table that he served. Throughout this question, our goal will be to predict `tip` using some or all of the other features in the DataFrame.\n",
    "\n",
    "Let's start by just using `total_bill` to predict `tip`. Here's a scatter plot showing the relationship between the two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips, x='total_bill', y='tip', title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions defined in the **Helper Functions** section make it easy to fit a linear prediction rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_one_feature = create_design_matrix(tips, ['total_bill'])\n",
    "y = tips['tip']\n",
    "\n",
    "# Notice that X_one_feature has two columns\n",
    "X_one_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding w*\n",
    "w_one_feature = solve_normal_equations(X_one_feature, y)\n",
    "w_one_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now use this prediction rule to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of an augmented feature vector for a total bill of 15 with the optimal parameter vector\n",
    "np.array([1, 15]) @ w_one_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips, x='total_bill', y='tip', title='Tip vs. Total Bill')\n",
    "\n",
    "x_range = np.linspace(0, 60)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = tips['total_bill'], y = y, mode = 'markers', name = 'actual'))\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_one_feature[0] + w_one_feature[1] * x_range, \n",
    "                         name = 'linear prediction rule', \n",
    "                         line=dict(color='red')))\n",
    "\n",
    "fig.update_layout(xaxis_title = 'Total Bill', yaxis_title = 'Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error of this prediction rule is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_one_feature = mean_squared_error(X_one_feature, y, w_one_feature)\n",
    "mse_one_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the DataFrame `prediction_rules` solely to keep track of the prediction rules we've used so far along with their MSEs. You will not need to interface with it at all in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rules = pd.DataFrame(index=['total_bill'], columns=['MSE'])\n",
    "prediction_rules.loc['total_bill'] = mse_one_feature\n",
    "\n",
    "prediction_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A â€“ Making predictions using the single-feature model (2 Points)\n",
    "\n",
    "\n",
    "Let's suppose Billy works for a day as a waiter at [Nobu San Diego](https://www.noburestaurants.com/sandiego/home/), a very expensive sushi restaurant. He waits a table whose total bill is \\$350. He decides to use the above linear prediction rule to predict the tip that he will receive.\n",
    "\n",
    "<p style=\"color:red\"><b>Your Job</b></p> What tip would the above single-feature model predict for a total bill of 350? Is this prediction likely to be accurate? Why or why not? Report your answers to these questions in your PDF writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prediction here.\n",
    "... # TODO (Hint: See the example prediction for a total bill of 15 above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B â€“ Using two features (5 Points)\n",
    "\n",
    "Now, let's suppose we want to use `total_bill` AND `table_size` to predict `tip`.\n",
    "\n",
    "<p style=\"color:red\"><b>Your Job</b></p> \n",
    "\n",
    "Below, complete the following tasks:\n",
    "\n",
    "- i. Assign `X_two_features` to the design matrix for this new prediction rule.\n",
    "- ii. Assign `w_two_features` to the optimal parameter vector for this new prediction rule. Write the resulting vector in your PDF writeup.\n",
    "- iii. Assign `mse_two_features` to the mean squared error of this new prediction rule. Write the result in your PDF writeup.\n",
    "- iv. Write the resulting prediction rule as a formula in your PDF writeup, using the numbers you found in task ii. The only variables in your formula should be `total_bill` and `table_size` (or, if you prefer, $x^{(1)}$ and $x^{(2)}$).\n",
    "- v. Did adding `table_size` as a feature make our prediction rule significantly more accurate as compared to the prediction rule that used just `total_bill`? How can you tell? Write your answer in your PDF writeup.\n",
    "\n",
    "Tasks i, ii, and iii should each only take line; remember to use the functions defined for you at the start of the notebook. This subpart should not take very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_features = ... # TODO\n",
    "w_two_features = ... # TODO\n",
    "w_two_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_two_features = ... # TODO\n",
    "mse_two_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, just run it\n",
    "prediction_rules.loc['total_bill, table_size'] = mse_two_features\n",
    "prediction_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you completed tasks i-iii correctly, you should see a scatter plot of the original data points and your prediction rule below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[0:60:2, 0:8:2]\n",
    "Z = w_two_features[0] + w_two_features[1] * XX + w_two_features[2] * YY\n",
    "plane = go.Surface(x=XX, y=YY, z=Z)\n",
    "\n",
    "fig = go.Figure(data=[plane])\n",
    "fig.add_trace(go.Scatter3d(x=tips['total_bill'], \n",
    "                           y=tips['table_size'], \n",
    "                           z=tips['tip'], mode='markers', marker = {'color': '#656DF1'}))\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = 'Total Bill',\n",
    "    yaxis_title = 'Table Size',\n",
    "    zaxis_title = 'Tip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C â€“ Comparing coefficients (2 Points)\n",
    "\n",
    "Which feature is more important in predicting tip â€“ `total_bill` or `table_size`?\n",
    "\n",
    "Assuming you answered Part B correctly, run the cell below to create a standardized design matrix, where the two columns for `total_bill` and `tip` are standardized to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_features_standardized = X_two_features.copy()\n",
    "X_two_features_standardized[:, 1:] = (X_two_features[:, 1:] - np.mean(X_two_features[:, 1:], axis=0)) / X_two_features[:, 1:].std(axis=0, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>Your Job</b></p> \n",
    "\n",
    "Determine `w_two_features_standardized`, the standardized regression coefficients for our two-feature prediction rule. In your PDF writeup, provide the value of `w_two_features_standardized` as well as which feature is more important for predicting `tip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_two_features_standardized = ... # TODO\n",
    "w_two_features_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D â€“ Using polynomial features (Points: 3)\n",
    "\n",
    "Let's revisit the scatter plot of tip vs. total bill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips, x='total_bill', y='tip', title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in class, let's see if using higher-degree polynomial features yields a more accurate prediction rule. Specifically, let's try and create a degree 4 polynomial prediction rule, using the features `total_bill`, `total_bill^2`, `total_bill^3`, and `total_bill^4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the tips DataFrame so that we don't modify the original data\n",
    "tips_with_poly_features = tips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing total_bill^2\n",
    "tips_with_poly_features['total_bill^2'] = tips_with_poly_features['total_bill']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>Your Job</b></p>\n",
    "\n",
    "- i. Add columns `total_bill^3` and `total_bill^4` to the DataFrame `tips_with_poly_features`. Provide a screenshot of your code in your PDF writeup.\n",
    "- ii. Define `X_poly`, `w_poly`, and `mse_poly` to be the design matrix, optimal parameter vector, and mean squared error of our new 4th degree polynomial prediction rule. In your PDF writeup, include the values of `w_poly` and `mse_poly`.\n",
    "- iii. Write the resulting prediction rule as a formula in your PDF writeup, using the numbers you found in task ii. The only variable in your formula should be `total_bill`, and powers of it (or $x$, if you prefer).\n",
    "\n",
    "Again, this subpart should only take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_with_poly_features['total_bill^3'] = ... # TODO\n",
    "tips_with_poly_features['total_bill^4'] = ... # TODO\n",
    "tips_with_poly_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = ... # TODO\n",
    "w_poly = ... # TODO\n",
    "w_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly = ... # TODO\n",
    "mse_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, just run it\n",
    "prediction_rules.loc['total_bill 4th degree poly'] = mse_poly\n",
    "prediction_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E â€“ Interpreting the model with polynomial features (Points: 2)\n",
    "\n",
    "Assuming you completed Part D correctly, run the following cell to see a visualization of our 4th degree polynomial prediction rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = tips['total_bill'], y = tips['tip'], mode = 'markers', name = 'actual'))\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_poly[0] + w_poly[1] * (x_range) + w_poly[2] * (x_range**2) + \\\n",
    "                             w_poly[3] * (x_range**3) + w_poly[4] * (x_range**4),\n",
    "                         name = '4th degree polynomial prediction rule', \n",
    "                         line=dict(color='#F7CF5D', width=5)))\n",
    "\n",
    "fig.update_layout(xaxis_title = 'Total Bill', yaxis_title = 'Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw, the 4th degree polynomial prediction rule seems to fit the data the best so far, since its MSE is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see what happens when we \"zoom out\" and look at how this prediction rule behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 70)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = tips['total_bill'], y = tips['tip'], mode = 'markers', name = 'actual'))\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_poly[0] + w_poly[1] * (x_range) + w_poly[2] * (x_range**2) + \\\n",
    "                             w_poly[3] * (x_range**3) + w_poly[4] * (x_range**4),\n",
    "                         name = '4th degree polynomial prediction rule', \n",
    "                         line=dict(color='#F7CF5D', width=5)))\n",
    "\n",
    "fig.update_layout(xaxis_title = 'Total Bill', yaxis_title = 'Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again suppose Billy works for a day as a waiter at [Nobu San Diego](https://www.noburestaurants.com/sandiego/home/). He waits a table whose total bill is \\$350. He decides to use the above 4th degree polynomial prediction rule to predict the tip that he will receive.\n",
    "\n",
    "<p style=\"color:red\"><b>Your Job</b></p> What tip would the above polynomial model predict for a total bill of 350? Why is a prediction rule with a lower MSE not necessarily better than a prediction rule with a higher MSE, as is the case here? Report your answers to these questions in your PDF writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prediction here.\n",
    "... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F â€“ Using categorical features (Points: 4)\n",
    "\n",
    "There was another column in our original DataFrame, `tip`, that we haven't yet looked at: `day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three possible values of `day`: `'Thur'`, `'Sat'`, and `'Sun'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(tips.groupby('day').count()['total_bill'].loc[['Thur', 'Sat', 'Sun']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike `total_bill` and `table_size`, `day` is **categorical**. This means there's no easy way to put it in our design matrix or find the best prediction rule.\n",
    "\n",
    "A naÃ¯ve solution would be to encode `'Thur'` as 1, `'Sat'` as 2, and `'Sun'` as 3, but this would make it seem like Sunday is \"more\" than Saturday or Thursday in some regard, which it is not â€“ these are all just different days of the week.\n",
    "\n",
    "A more robust and common solution is called **one-hot encoding** (OHE). You will be exposed to it in more detail in DSC 80, but we want to show you an example of how it works now since it's a natural extension of what we've already covered.\n",
    "\n",
    "Let's first get it working on a toy example. Let's pretend we have a DataFrame with just 5 rows and 2 columns, `total_bill` and `day`. Call it `mini_tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about what this code is doing, just run the cell.\n",
    "mini_tips = pd.DataFrame()\n",
    "mini_tips['total_bill'] = tips['total_bill'].iloc[:5]\n",
    "mini_tips['day'] = ['Sat', 'Sun', 'Sun', 'Thur', 'Sat']\n",
    "\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we **one-hot encode** a categorical variable, we create a new column for each unique value of that categorical variable. In this case, we'd create three new columns, one each for `'Thur'`, `'Sat'`, and `'Sun'`.\n",
    "\n",
    "Each of these new columns is binary, meaning they only contain the values 1 and 0. \n",
    "- The new column for `'Thur'`, which we'll call `is_thur`, will contain a 1 for rows where the value of `day` is `'Thur'`, and 0 for all other rows. \n",
    "- Similarly, the new column for `'Sun'`, which we'll call `is_sun`, will contain a 1 for rows where the value of day is `'Sun'`, and 0 for all other rows.\n",
    "\n",
    "Again, you'll see more efficient ways to do this in later courses, but here's one way to one-hot encode using a technique you saw in DSC 10 â€“ boolean comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(mini_tips['day'] == 'Thur')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_tips['is_thur'] = (mini_tips['day'] == 'Thur').astype(int)\n",
    "mini_tips['is_sat'] = (mini_tips['day'] == 'Sat').astype(int)\n",
    "mini_tips['is_sun'] = (mini_tips['day'] == 'Sun').astype(int)\n",
    "\n",
    "# Dropping the day column. We've encoded it numerically, we don't need it anymore.\n",
    "mini_tips = mini_tips.drop(columns=['day'])\n",
    "\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've converted a categorical feature into three numerical features, so we're good to go!\n",
    "\n",
    "**There's just one more thing.** Since we're used to fitting linear prediction rules with an intercept term, our design matrix generally has a column of all 1s in it. In the case of `mini_tips`, which contains three binary columns, this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_design_matrix(mini_tips, list(mini_tips.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This design matrix contains redundant information! Specifically, we can recreate the column of all 1s by adding together the three one-hot encoded columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the 0, 1, 2, 3, 4 that you see is the index of this Series, which is irrelevant for our purposes\n",
    "mini_tips['is_thur'] + mini_tips['is_sat'] + mini_tips['is_sun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that our design matrix $X$ suffers from multicollinearity, and is not **full rank**. There are multiple nasty side effects of this â€“ there is no unique solution for $\\vec{w}^*$ and it makes our optimal parameters more difficult to interpret.\n",
    "\n",
    "You'll explore this problem in later statistics and data science courses, so don't worry if this is a bit confusing. **For now, know this â€“ the way to avoid this problem is to drop one of the one-hot encoded columns.** That way, there is no redundant information in the design matrix, and we don't run into any issues. (As you'll see later on, this is not \"getting rid\" of any information, so it will not impact our predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've arbitrarily chosen to drop is_thur, but it would make no difference if we instead dropped is_sat or is_sun.\n",
    "mini_tips = mini_tips.drop(columns=['is_thur'])\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_design_matrix(mini_tips, list(mini_tips.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a design matrix that is ready to go. Let's replicate this process on our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "tips_ohe = tips.copy()\n",
    "tips_ohe['is_sat'] = (tips_ohe['day'] == 'Sat').astype(int)\n",
    "tips_ohe['is_sun'] = (tips_ohe['day'] == 'Sun').astype(int)\n",
    "\n",
    "# Design matrix with two one-hot encoded columns.\n",
    "X_ohe = create_design_matrix(tips_ohe, ['total_bill', 'is_sat', 'is_sun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ohe = solve_normal_equations(X_ohe, y)\n",
    "w_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the resulting prediction rule. We've zoomed into the region where the total bills are less than 30 to make the prediction rule more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 30)\n",
    "\n",
    "under_30 = tips[tips['total_bill'] < 30]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = under_30['total_bill'], y = under_30['tip'], mode = 'markers', name = 'actual'))\n",
    "\n",
    "# Line for Thursday\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_ohe[0] + w_ohe[1] * x_range, \n",
    "                         name = 'Thursday', \n",
    "                         line=dict(color='red')))\n",
    "\n",
    "# Line for Saturday\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_ohe[0] + w_ohe[2] + w_ohe[1] * x_range, \n",
    "                         name = 'Saturday', \n",
    "                         line=dict(color='gold')))\n",
    "\n",
    "# Line for Sunday\n",
    "fig.add_trace(go.Scatter(x = x_range, \n",
    "                         y = w_ohe[0] + w_ohe[3] + w_ohe[1] * x_range, \n",
    "                         name = 'Sunday', \n",
    "                         line=dict(color='green')))\n",
    "\n",
    "fig.update_layout(xaxis_title = 'Total Bill', yaxis_title = 'Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the prediction rule is actually three separate lines, each of which have the same slope but different intercepts!\n",
    "\n",
    "Let's try and understand why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction rule is of the following form:\n",
    "\n",
    "$$\\text{predicted tip} = 0.908 + 0.105 (\\text{total bill}) - 0.069 (\\text{is saturday}) + 0.091 (\\text{is sunday})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>Your Job</b></p>\n",
    "\n",
    "- What is the intercept of the line for when `day` is Thursday?\n",
    "- What is the intercept of the line for when `day` is Saturday?\n",
    "- What is the intercept of the line for when `day` is Sunday?\n",
    "\n",
    "Write the numerical answers for all three questions in your writeup PDF. That is the only action item you have for Part F, but please ask questions if any of this subpart was unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for completeness, we'll also compute the MSE of this prediction rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ohe = mean_squared_error(X_ohe, y, w_ohe)\n",
    "prediction_rules.loc['total_bill + OHE day'] = mse_ohe\n",
    "prediction_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new prediction rule didn't have a much lower MSE than the prediction rule that used `total_bill` only. That's not all that surprising, since the three lines above look quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for Problem 2! We hope you now have a better understanding of multiple linear regression and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement for Problem 3\n",
    "\n",
    "Note that this question is entirely in the PDF of the homework. The code we've written here just serves to help you understand that the sum of the residuals when we have an intercept term is truly 0, using the data from Problem 2 as an example. Feel free to experiment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y - X_one_feature @ w_one_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y - X_two_features @ w_two_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y - X_two_features @ w_two_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 â€“ Least Absolute Deviations Regression\n",
    "\n",
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're providing you with all of the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_combinations(data, k=2):\n",
    "    \"\"\"Returns the unique sets of length k from the dataset.\"\"\"\n",
    "    return list(combinations(data, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane_mae(a, b, c, data):\n",
    "    \"\"\"Computes the mean absolute error for a given plane.\"\"\"\n",
    "    loss = 0\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        x_i, y_i, z_i = data[i]\n",
    "        loss += abs(z_i - (a * x_i + b * y_i + c))\n",
    "    return loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_plane(planes, data):\n",
    "    \"\"\"Finds the best plane given a list of planes and the dataset.\"\"\"\n",
    "    lowest_mae = float(\"inf\")\n",
    "    best_plane = None\n",
    "\n",
    "    for plane in planes:\n",
    "        a, b, c = plane\n",
    "        mae = plane_mae(a, b, c, data)\n",
    "        if mae < lowest_mae:\n",
    "            lowest_mae = mae\n",
    "            best_plane = plane\n",
    "\n",
    "    return best_plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll use the tips dataset. We'll use `total_bill` and `table_size` to predict `tip`, as we did in Problem 2. Below, we create a matrix in the form that the above functions expect. The first column, $x$, contains total bills, the second column, $y$, contains table sizes, and the third column, $z$, contains tips.\n",
    "\n",
    "(Note that we're also only taking the first 50 rows of `tips`, since the process we're going to implement is very slow.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_for_p4 = tips[['total_bill', 'table_size', 'tip']].iloc[:50].values.tolist()\n",
    "tips_for_p4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code `generate_all_combinations(tips_for_p4, 3)` will return the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_combinations(tips_for_p4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>Your Job</b></p>\n",
    "\n",
    "Complete the implementation of the function `generate_all_planes`, which takes in a list of point triplets in the above format and returns a list of $(a, b, c)$ triplets, such that the $i$th $(a, b, c)$ triplet defines a plane that contains three points in the $i$th element of the list `triplets`.\n",
    "\n",
    "For instance, the first element of `generate_all_planes(generate_all_combinations(tips_for_p4, 3))` should be an $(a, b, c)$ triplet defining the plane $z = ax + by + c$ that passes through the three points `[16.99, 2.0, 1.01], [10.34, 3.0, 1.66], [21.01, 3.0, 3.5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_planes(triplets):\n",
    "    '''Returns an (a, b, c) triplet defining a plane for every triplet of points in the list triplets.'''\n",
    "    planes = []\n",
    "    for triplet in triplets:\n",
    "        A, B, C = np.array(triplet) # Unpacks all three points\n",
    "        A_to_B = ... # TODO (Hint: see the hint in the PDF for part (a))\n",
    "        A_to_C = ... # TODO (Hint: see the hint in the PDF for part (a))\n",
    "        normal = ... # TODO (Hint: This should be a vector normal to the plane. Look into np.cross.)\n",
    "        point = np.dot(normal, A)\n",
    "        x, y, z = normal\n",
    "        a, b, c = ... # TODO (Hint: Try determining what a, b, and c should be on paper first.)\n",
    "        planes.append((a, b, c))\n",
    "    return planes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented `generate_all_planes` correctly, the following cell should print out the best $a, b, c$ triplet (i.e. the LAD plane) for the tips dataset. You may see some `RuntimeWarning`s; you can ignore those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = generate_all_combinations(tips_for_p4, 3)\n",
    "planes = generate_all_planes(triplets)\n",
    "a_best, b_best, c_best = find_best_plane(planes, tips_for_p4)\n",
    "print(\"The best (a, b, c) triplet for the tips data is\", (a_best, b_best, c_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of the resulting plane. Remember that we only used the first 50 rows of `tips` to fit this prediction rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[0:60:2, 0:8:2]\n",
    "Z = c_best + a_best * XX + b_best * YY\n",
    "plane = go.Surface(x=XX, y=YY, z=Z)\n",
    "fig = go.Figure(data=[plane])\n",
    "fig.add_trace(go.Scatter3d(x=np.array(tips_for_p4)[:, 0], \n",
    "                           y=np.array(tips_for_p4)[:, 1], \n",
    "                           z=np.array(tips_for_p4)[:, 2], mode='markers', marker = {'color': '#656DF1'}))\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = 'Total Bill',\n",
    "    yaxis_title = 'Table Size',\n",
    "    zaxis_title = 'Tip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! ðŸ’ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
